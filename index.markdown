---
layout: home
---

<style>
td, th, tr, table {
   border: none!important;
   background-color: transparent!important;
}
h2, h3, h4 {
    text-align: center;
}
</style>

# About

The UBC/SFU Joint Statistics Seminar is jointly hosted by the graduate students of the [UBC Department of Statistics](https://www.stat.ubc.ca/) and the [SFU Department of Statistics and Actuarial Science](https://www.sfu.ca/stat-actsci.html). The Spring 2022 event is the second of two events taking place in the 2021/2022 academic year. The Fall 2021 event was organized by graduate students from SFU, and the Spring 2022 event is organized by graduate students from UBC. Over its 17-year history, the event has offered Statistics and Actuarial Science graduate and undergraduate students at both schools an opportunity to network with their peers and to attend accessible talks about the research work of their fellow students and faculty. 

The Spring 2022 event includes talks given by six students (three from UBC and three from SFU) and one faculty member from UBC.

Check out more events hosted by the [UBC Statistics Graduate Student Association](https://ubc-stat-grad.github.io/).

# Registration

This term's event will be hosted virtually through zoom on **March 12, 2022** starting at **9:30am**. Register now through the [registration form](https://docs.google.com/forms/d/e/1FAIpQLSdrzTMJV_TtChGLx7nOWhIXPVXnizvYXl_gxAcf2ixENXBBOA/viewform).

# Schedule

---

### <span style="color:#1756a9"> Welcome Message </span> 
#### <span style="color:#1756a9">  9:30am - 9:35am </span>

---

### <span style="color:#1756a9"> Xiaomeng Ju (UBC) </span>
#### <span style="color:#1756a9"> 9:35am - 10:00am </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/XiaomengJu.jpeg">
</td><td>
<p><strong>Tree-based boosting with functional data</strong></p>
<p style="text-align:justify">We present a robust regression estimator for models with a functional explanatory variable and a scalar response. The algorithm uses decision trees constructed with multiple projections as the "base-learners", which we call "functional multi-index trees”. Compared with widely studied single-index estimators, our multi-index estimator enables modelling possible interactions between indices, allowing us to better approximate complex regression functions. The effectiveness of our method is illustrated using a numerical experiment that compares it with several linear and nonlinear regression estimators, including recently proposed nonparametric and semiparametric functional additive estimators.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Elijah Cavan (SFU) </span>
#### <span style="color:#1756a9"> 10:00am - 10:25am </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/ElijahCavan.png">
</td><td>
<p><strong>Optimal Roster Construction In Sports</strong></p>
<p style="text-align:justify">In Major League Sports (MLB, NBA, NHL, NFL), it is the Front Office Staff (FOS) who make decisions about who plays for their respective team. The FOS bear the brunt of the responsibility for acquiring players through drafting, trading and signing players in free agency while typically contesting with maximum roster salary constraints. The players themselves are volatile assets of these teams- their value fluctuates with age and performance. A simple comparison can be made when viewing players as assets. The problem here is similar to that of optimizing your investment portfolio. The goal is ultimately to maximize your periodic returns while tolerating a fixed risk (degree of uncertainty/ potential loss). Each franchise may value assets differently, and some may only tolerate lower risk levels- these are examples of factors that introduce additional constraints into the model. In this talk we will detail the mathematical formulation of this problem as a constrained optimization problem- which can be solved with classical machine learning methods, but is also well posed as a problem to be solved on quantum computers.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Nikola Surjanovic (UBC) </span>
#### <span style="color:#1756a9"> 10:25am - 10:50am </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/NikolaSurjanovic.jpg">
</td><td>
<p><strong>Music Theory: Mathematical Results and Curiosities</strong></p>
<p style="text-align:justify">Note: I have decided to present a fun, math-related talk instead of a formal “research” talk at this year’s event. I hope there will be something new for everyone to enjoy.</p>
<p style="text-align:justify">What does the fundamental theorem of arithmetic have to do with music? What does it mean to play something “in tune” and can this notion be formalized to a certain extent? As mathematicians and statisticians, we are in a fortunate position to be able to understand some of these concepts. I plan to present a brief overview of music theory and some curious mathematical results that will (hopefully) surprise most people. Some possible topics include: the fundamental theorem of arithmetic and Pythagorean tuning, the twelfth root of two and equal temperament, and differential equations and overtones.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Break </span>
#### <span style="color:#1756a9"> 10:50am - 11:00am </span>

---

### <span style="color:#1756a9"> Wendy Wang (SFU) </span>
#### <span style="color:#1756a9"> 11:00am - 11:25am </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/WendyWang.jpeg">
</td><td>
<p><strong>Selection Between Variance-Optimal and Bias-Optimal Designs When Some Two-Factor Interactions Are Important</strong></p>
<p style="text-align:justify">Fractional factorial designs are widely used in many fields of studies because they allow us to study the effects of many factors on the response. However, as the primary interest of most experiments is for screening important factors, interactions are generally assumed to be negligible. When some two-factor interactions are important, we consider two types of designs which are namely, variance-optimal designs and bias-optimal designs. In our study, we compare these two types of designs by using a mean squared error criterion that takes effect sparsity into consideration. We obtain a close-form expression of this mean squared error criterion for the two types of designs. Under different levels of sparsity, comparison results are obtained for designs of 10, 12, 14, 20, 26, 28 runs, which will help practitioners to choose between the two types of designs.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Quanhan (Johnny) Xi (UBC) </span>
#### <span style="color:#1756a9"> 11:25am - 11:50am </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/JohnnyXi.jpg">
</td><td>
<p><strong>Identification of Latent Variables in Generative Modeling</strong></p>
<p style="text-align:justify">Generative models aim to fit data by passing simple latent factors through complex generator functions. Leveraging modern functional parametrizations such as deep neural networks, generative modeling has excelled on tasks such as generating synthetic images and reconstruction with relatively simple prior latent distributions. In image data, for example, the latent factors are of much lower dimension than the pixel space, and we also wish to perform inference on them as a form of dimension reduction in hopes of recovering higher-level factors of variation. Recently, there has been an effort to examine the latent variable identifiability of these models equipped with the flexibility of arbitrary, non-linear functions. In this talk, we give a characterization of the indeterminacies of a general class of generative models, from which many existing results may be readily deduced. Armed with insights from our results, we propose a strongly identifiable generative model with a fully flexible generator by leveraging data arising from multiple environments. We show a proof-of-concept implementation of a linear generator, in a modern take on factor analysis.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Mandy Yao (SFU) </span>
#### <span style="color:#1756a9"> 11:50am - 12:15pm </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/MandyYao.jpeg">
</td><td>
<p><strong>Effect of numerical errors on the convergence and accuracy of Markov Chains</strong></p>
<p style="text-align:justify">Markov Chain Monte Carlo (MCMC) algorithms are a class of methods that is widely used to draw samples from a probability distribution and draw inferences. In particular, it constructs a stochastic process whose limiting distribution is the unknown distribution of interest in a given problem. For example, it can be used for Bayesian Calibration, which is the problem of determining a distribution for parameters in a physical model from noisy observations on the output of the model using a Bayesian approach. In practice, there could be numerical errors in the computer simulations that affect the MCMC samples, and this could have a significant effect on convergence and accuracy. To study this, we use some existing theory on the convergence of perturbed stable Markov Chains, which works with a “roundoff error” that arises from the finite precision of computer computation. We adapt this theory to derive some conditions that guarantee the convergence of MCMC algorithms when a more general model for computational error is used. We also quantify the effect of error on the accuracy of the MCMC samples using a time series model.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Break </span>
#### <span style="color:#1756a9"> 12:15pm - 12:25pm </span>

---

### <span style="color:#1756a9"> Faculty Speaker: Keegan Korthauer </span>
#### <span style="color:#1756a9"> 12:25pm - 1:25pm </span>

<table cellspacing="0" cellpadding="0">
<tr><td width="50%" style="vertical-align:text-top">
<img src="assets/KeeganKorthauer.jpeg">
</td><td>
<p><strong>Plugging the leaky pipeline in Statistics</strong></p>
<p style="text-align:justify">The “leaky pipeline” is a metaphor often used to describe the lack of women making career advancements in STEM fields. The field of statistics is not immune to this phenomenon. Although women currently earn graduate degrees in statistics at similar rates to men, they are underrepresented in senior roles and leadership positions in academia. In this talk, I will describe the visible and invisible barriers that were overcome by pioneering women in statistics, outline some of the major factors that continue to perpetuate the leaky pipeline, and discuss its outlook for the statistics community. My goal is for you to come away with (1) awareness and recognition of the challenge of retaining women in STEM fields, including statistics, and (2) concrete strategies to help repair the leaky pipeline in our field.</p>
<p>Link to the slides can be found <a href="//bit.ly/leakypipeline-statistics" style="color:#1756a9">here</a>.</p>
</td></tr>
</table>

---

### <span style="color:#1756a9"> Networking </span>
#### <span style="color:#1756a9"> 1:25pm </span>

---

# Sponsors

|<img src="assets/stat.jpg" width=200/>|<img src="assets/canssi.png" width=200/>|<img src="assets/gss.png" width=200/>|

# Past Seminars

|[Fall 2021](https://www.sfu.ca/~rennyd/JointSeminar2021/) | [Spring 2021](https://www.stat.ubc.ca/~kenny.chiu/jointseminar/spring2021/) | [Fall 2020](http://www.sfu.ca/~nsurjano/JointSeminar/) | [Spring 2020](https://chiukenny.github.io/jointseminar-2019w2/) |